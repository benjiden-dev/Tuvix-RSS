# TuvixRSS Deployment Guide

This guide covers development and release processes for both Docker Compose and Cloudflare (Workers + Pages) deployments.

## Table of Contents

- [Overview](#overview)
- [Docker Compose Deployment](#docker-compose-deployment)
- [Cloudflare Deployment](#cloudflare-deployment)
- [Shared Topics](#shared-topics)
  - [Environment Variables](#environment-variables)
  - [Database Migrations](#database-migrations)
  - [Scheduled Tasks (Cron)](#scheduled-tasks-cron)
- [CI/CD Integration](#cicd-integration)
- [Security Checklist](#security-checklist)
- [Performance Optimization](#performance-optimization)

---

## Overview

TuvixRSS supports two deployment targets:

1. **Docker Compose** - Traditional container-based deployment with Node.js runtime
2. **Cloudflare** - Serverless edge deployment:
   - **API**: Cloudflare Workers (serverless edge runtime) - typically deployed to `api.example.com`
   - **Frontend**: Cloudflare Pages (static site hosting) - typically deployed to `feed.example.com`

Both deployments share the same codebase with runtime-specific adapters.

**Example Domain Structure:**
- `example.com` - Static blog (optional, separate from TuvixRSS)
- `feed.example.com` - Frontend Pages app (TuvixRSS UI)
- `api.example.com` - Worker API (TuvixRSS backend)

### Architecture Differences

| Feature | Docker Compose | Cloudflare Workers |
|---------|---------------|-------------------|
| Runtime | Node.js 20+ | Cloudflare Workers |
| Database | SQLite (better-sqlite3) | D1 (Cloudflare's SQLite) |
| Cron | node-cron | Workers Scheduled Events |
| Rate Limiting | Disabled | Cloudflare Workers rate limit bindings |

### Authentication

TuvixRSS uses **Better Auth** for authentication, which manages user sessions via HTTP-only cookies. The `BETTER_AUTH_SECRET` environment variable (minimum 32 characters) is used to sign and verify session cookies securely. No JWT tokens are used.

**Local Development**: Better Auth works perfectly with localhost setups:
- Frontend and API can run on different ports (e.g., `localhost:5173` and `localhost:3001`)
- Cookies are automatically handled (localhost domain works for both ports)
- CORS is configured with `credentials: true` to allow cookies

**Cross-Subdomain**: If your frontend and API are on different subdomains (e.g., `feed.example.com` and `api.example.com`), configure the `COOKIE_DOMAIN` secret to the root domain (e.g., `example.com`) to enable cross-subdomain cookies.

---

## Docker Compose Deployment

### Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- Git

### Development Setup

#### 1. Initial Setup

```bash
# Clone repository
git clone https://github.com/yourusername/TuvixRSS.git
cd TuvixRSS

# Copy environment file
cp .env.example .env

# Generate secure Better Auth secret
openssl rand -base64 32

# Edit .env file with your values
vim .env
```

**Required Environment Variables:**

```env
BETTER_AUTH_SECRET=your-generated-secret-here  # Min 32 chars
CORS_ORIGIN=http://localhost:5173
DATABASE_PATH=/app/data/tuvix.db
PORT=3001
NODE_ENV=production
```

#### 2. Build and Run

```bash
# Build Docker images
pnpm run docker:build

# Start containers
pnpm run docker:up

# View logs
pnpm run docker:logs

# Stop containers
pnpm run docker:down
```

**Services:**
- `api` - tRPC API server (port 3001)
  - Health check: http://localhost:3001/health
  - tRPC endpoint: http://localhost:3001/trpc
- `app` - React frontend (port 5173)
  - Health check: http://localhost:5173/health

#### 3. Local Development (Without Docker)

For active development without Docker:

```bash
# Install dependencies
pnpm install

# Run database migrations
pnpm run db:migrate

# Start both API and frontend
pnpm run dev

# Or start separately:
pnpm run dev:api    # API on :3001
pnpm run dev:app    # Frontend on :5173
```

#### 4. Testing

```bash
# Run all tests
pnpm run test

# Run tests with coverage
pnpm run test:coverage

# Type checking
pnpm run type-check

# Linting
pnpm run lint

# Pre-release checks
pnpm run pre-check
```

### Production Deployment

#### 1. Server Setup

```bash
# On your production server
git clone https://github.com/yourusername/TuvixRSS.git
cd TuvixRSS

# Create production environment file
cp .env.example .env
vim .env
```

**Production Environment Variables:**

```env
# SECURITY: Use strong secrets in production
BETTER_AUTH_SECRET=<generate-with-openssl-rand-base64-32>  # Min 32 chars
CORS_ORIGIN=https://feed.example.com  # Frontend URL (or multiple origins comma-separated)
DATABASE_PATH=/app/data/tuvix.db
PORT=3001
NODE_ENV=production

# Optional: Customize fetch behavior
FETCH_INTERVAL_MINUTES=60  # How often to fetch RSS feeds
```

#### 2. Deploy

```bash
# Build and start
docker compose build
docker compose up -d

# Verify health
curl http://localhost:3001/health
curl http://localhost:5173/health

# Monitor logs
docker compose logs -f
```

#### 3. Database Backups

```bash
# Backup database
docker compose exec api cp /app/data/tuvix.db /app/data/backup-$(date +%Y%m%d).db

# Or from host (if volume is mounted)
cp ./data/tuvix.db ./data/backup-$(date +%Y%m%d).db
```

#### 4. Updates

```bash
# Pull latest code
git pull origin main

# Rebuild and restart
docker compose down
docker compose build
docker compose up -d

# Database migrations run automatically on startup
```

### Configuration

#### Dockerfile Structure

**API Dockerfile** (`packages/api/Dockerfile`):
- Multi-stage build (builder + production)
- Installs pnpm 10.19.0
- Runs migrations on startup
- Exposes port 3001
- Health check on /health endpoint

**App Dockerfile** (`packages/app/Dockerfile`):
- Multi-stage build with nginx
- Accepts VITE_API_URL build arg
- SPA routing support
- Static asset caching
- Health check on /health endpoint

#### docker-compose.yml

```yaml
services:
  api:
    build: ./packages/api
    ports:
      - "3001:3001"
    volumes:
      - ./data:/app/data
    environment:
      - DATABASE_PATH=/app/data/tuvix.db
      - BETTER_AUTH_SECRET=${BETTER_AUTH_SECRET}
      - CORS_ORIGIN=${CORS_ORIGIN}
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3001/health"]
      interval: 30s
      timeout: 3s
      retries: 3

  app:
    build:
      context: ./packages/app
      args:
        - VITE_API_URL=${VITE_API_URL:-http://localhost:3001/trpc}
    ports:
      - "5173:80"
    depends_on:
      api:
        condition: service_healthy
```

### Monitoring & Troubleshooting

#### Health Checks

```bash
# Check container health
docker compose ps
docker inspect tuvix-api | grep -A 5 Health

# Health check endpoints
curl http://localhost:3001/health
# Response: {"status":"ok","runtime":"nodejs"}

curl http://localhost:5173/health
# Response: ok
```

#### View Logs

```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f api

# Last 100 lines
docker compose logs --tail=100 api

# Filter cron logs
docker compose logs -f api | grep "RSS fetch\|Prune"
```

#### Common Issues

**Port Already in Use:**
```bash
# Check what's using the port
lsof -i :3001

# Change port in .env
PORT=3002
```

**Database Locked:**
```bash
# Stop all containers
docker compose down

# Remove stale lock
rm -f ./data/tuvix.db-shm ./data/tuvix.db-wal

# Restart
docker compose up -d
```

---

## Cloudflare Deployment

### Prerequisites

- Node.js 20+
- pnpm
- Cloudflare account ([Sign up](https://dash.cloudflare.com/sign-up))
- Wrangler CLI (`npm install -g wrangler` or use `npx wrangler`)

### Quick Start

```bash
# 1. Authenticate
npx wrangler login

# 2. Setup API (Workers)
cd packages/api
# Follow "API Setup" section below

# 3. Setup Frontend (Pages)
cd packages/app
# Follow "Frontend Setup" section below
```

### API Setup (Cloudflare Workers)

#### Step 1: Create Cloudflare Resources

**Authenticate:**
```bash
npx wrangler login
```

**Create D1 Database:**
```bash
cd packages/api

# Create database
npx wrangler d1 create tuvix

# Output will show:
# âœ… Successfully created DB 'tuvix'!
# database_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  # Copy this ID
#
# For local development: Create wrangler.toml.local with this ID
# For CI/CD: Add this ID as D1_DATABASE_ID GitHub secret
```

#### Step 2: Configure wrangler.toml

The `wrangler.toml` file is already configured with environment variable placeholders:

```toml
name = "tuvix-api"
main = "src/adapters/cloudflare.ts"
compatibility_date = "2024-11-10"
compatibility_flags = ["nodejs_als"]

# D1 Database binding
[[d1_databases]]
binding = "DB"
database_name = "tuvix"
database_id = "${D1_DATABASE_ID}"  # Substituted by envsubst in CI/CD

# Rate Limit Bindings
[[ratelimits]]
name = "API_RATE_LIMIT"
namespace_id = "1001"  # User-defined positive integer
simple = { limit = 10000, period = 60 }

[[ratelimits]]
name = "FEED_RATE_LIMIT"
namespace_id = "1002"  # User-defined positive integer
simple = { limit = 10000, period = 60 }

[vars]
RUNTIME = "cloudflare"

[triggers]
crons = ["*/5 * * * *"]  # Every 5 minutes
```

**Security Notes:**
- âœ… **Safe to commit**: Environment variable placeholders, structure, names, bindings
- âŒ **Never commit**: Filled-in `database_id` values (account-specific)
- ðŸ”’ **Use GitHub Secrets** (not Variables) for sensitive data

**Local Development Setup:**

Wrangler doesn't support environment variable substitution in `wrangler.toml`. For local development, create `wrangler.toml.local`:

```bash
cd packages/api

# Copy the example file
cp wrangler.toml.local.example wrangler.toml.local

# Edit wrangler.toml.local and replace "your-database-id-here" with your actual D1 database ID
# Example: database_id = "7078240d-69e3-46fb-bb21-aa8e5208de9b"
```

**Note:** `wrangler.toml.local` is gitignored and will override values in `wrangler.toml`.

**CI/CD Setup:**

The GitHub Actions workflow automatically substitutes `${D1_DATABASE_ID}` in `wrangler.toml` before deployment using `envsubst`.

**To configure:**
1. Go to your GitHub repository â†’ Settings â†’ Secrets and variables â†’ Actions
2. Click the **"Secrets"** tab (not "Variables")
3. Click "New repository secret"
4. Name: `D1_DATABASE_ID` (must match exactly)
5. Value: Your D1 database ID (from `wrangler d1 create tuvix`)
6. Click "Add secret"

**Why Secrets instead of Variables?**
- **Secrets** are encrypted and masked in logs (use for sensitive data)
- **Variables** are plain text and visible in logs (use for non-sensitive configuration)

#### Step 3: Set Secrets

**Required Secrets:**

```bash
cd packages/api

# Better Auth secret (min 32 chars)
npx wrangler secret put BETTER_AUTH_SECRET
# Generate with: openssl rand -base64 32

# CORS origin (frontend URL) - Set BEFORE deploying API
npx wrangler secret put CORS_ORIGIN
# Enter: https://feed.example.com (if using custom domain)
# Or: https://your-pages-project.pages.dev (if using Pages default)
# Multiple origins: https://feed.example.com,https://your-pages-project.pages.dev
```

**Optional Secrets:**

```bash
# Email service (Resend)
# See docs/developer/email-system.md for complete email setup guide
npx wrangler secret put RESEND_API_KEY
npx wrangler secret put EMAIL_FROM
npx wrangler secret put BASE_URL

# Cross-subdomain cookies (if frontend/API on different subdomains)
npx wrangler secret put COOKIE_DOMAIN
# Enter: example.com  (root domain, not subdomain like api.example.com)

# Admin initialization (first deployment only)
npx wrangler secret put ADMIN_USERNAME
npx wrangler secret put ADMIN_PASSWORD
npx wrangler secret put ADMIN_EMAIL
```

#### Step 4: Database Migrations

**Note:** For first deployment, run migrations BEFORE deploying. For subsequent deployments, migrations can run before or after deployment (CI/CD runs them after deployment).

```bash
cd packages/api

# Generate migrations from schema changes (if schema was modified)
pnpm run db:generate

# Apply migrations to production D1
pnpm run db:migrate:d1

# Verify migrations
npx wrangler d1 execute tuvix --remote \
  --command "SELECT name FROM sqlite_master WHERE type='table';"
```

#### Step 5: Deploy API

```bash
cd packages/api

# Pre-deployment checks
pnpm run type-check
pnpm run test
pnpm run build

# Deploy to Workers
pnpm run deploy
# Or: npx wrangler deploy

# Monitor deployment
npx wrangler tail
```

#### Step 6: Initialize Admin User (First Deployment)

After first deployment:

```bash
# Trigger admin initialization
curl -X POST https://your-worker.workers.dev/_admin/init

# Or if using custom domain:
curl -X POST https://api.example.com/_admin/init
```

**Example:**
```bash
curl -X POST https://api.tuvix.app/_admin/init
```

**Note**: Admin initialization creates a user in Better Auth's `user` table. You can then log in using Better Auth's `/api/auth/sign-in/username` endpoint.

### Frontend Setup (Cloudflare Pages)

#### Step 1: Create Pages Project

**Option A: Via Wrangler CLI (Recommended)**

```bash
# Create the Pages project (first time only)
npx wrangler pages project create tuvix-app

# Build and deploy
cd packages/app
export VITE_API_URL=https://api.example.com/trpc
# Or if not using custom domain: https://your-worker.workers.dev/trpc
pnpm run build
npx wrangler pages deploy dist --project-name=tuvix-app
```

**Note:** The project name (`tuvix-app` in this example) must match the `CLOUDFLARE_PAGES_PROJECT_NAME` GitHub secret used in CI/CD. This is the internal Cloudflare project name, not your custom domain.

**Option B: Via Cloudflare Dashboard**

1. Go to [Cloudflare Dashboard â†’ Pages](https://dash.cloudflare.com/pages)
2. Click **"Create a project"**
3. Connect your Git repository (GitHub/GitLab)
4. Configure build settings:
   - **Build command**: `cd packages/app && pnpm install && pnpm build`
   - **Build output directory**: `packages/app/dist`
   - **Root directory**: `/` (project root)
5. Add environment variable:
   - **Variable**: `VITE_API_URL`
   - **Value**: `https://api.example.com/trpc` (or `https://your-worker.workers.dev/trpc` if not using custom domain)

#### Step 2: Configure Custom Domain (Optional)

1. In Cloudflare Dashboard â†’ Pages â†’ Your Project â†’ **Custom domains**
2. Click **"Set up a custom domain"**
3. Enter your domain (e.g., `feed.example.com`)
4. Cloudflare will automatically configure DNS

**Update CORS:** After adding a custom domain, update the `CORS_ORIGIN` secret in your Worker to include the frontend URL:

```bash
npx wrangler secret put CORS_ORIGIN
# Enter: https://feed.example.com
```

**Example:** For `feed.tuvix.app`, set CORS_ORIGIN to `https://feed.tuvix.app`

### Cross-Subdomain Configuration

**When to configure:** After both API and Frontend are deployed and working.

If your frontend and API are on different subdomains (e.g., `feed.example.com` and `api.example.com`), configure cross-subdomain cookies:

**When You Need This:**
- âœ… Frontend on `feed.example.com`, API on `api.example.com`
- âœ… Frontend on `www.example.com`, API on `api.example.com`
- âŒ Both on same domain (e.g., `example.com/feed` and `example.com/api`)
- âŒ Both on same subdomain (e.g., `feed.example.com/feed` and `feed.example.com/api`)

**Configuration:**

```bash
# Set cookie domain to root domain (not subdomain)
npx wrangler secret put COOKIE_DOMAIN
# Enter: example.com  (NOT api.example.com or feed.example.com)
```

**Example:** For `feed.tuvix.app` and `api.tuvix.app`, set COOKIE_DOMAIN to `tuvix.app`

**Security Note**: Setting `COOKIE_DOMAIN` makes cookies accessible across all subdomains. Only enable if necessary and ensure all subdomains are trusted.

### Development Workflow

**Local Development:**

```bash
# API (Workers)
cd packages/api
pnpm run dev:workers
# Starts: Local Workers runtime (Miniflare), Local D1 database, Auto-reload

# Frontend
cd packages/app
pnpm run dev
# Frontend runs on http://localhost:5173
# Points to API at VITE_API_URL (default: http://localhost:3001/trpc)

# Testing
pnpm run test
npx wrangler dev  # Test Workers locally
npx wrangler dev --test-scheduled  # Test cron trigger locally
```

**Production Deployment:**

```bash
# Pre-Deployment
pnpm run type-check
pnpm run test

# Deploy API
cd packages/api
pnpm run db:migrate:d1  # Run migrations
pnpm run build
pnpm run deploy

# Deploy Frontend
cd packages/app
export VITE_API_URL=https://api.example.com/trpc
pnpm run build
npx wrangler pages deploy dist --project-name=tuvix-app

# Verify
curl https://api.example.com/health
curl https://feed.example.com/health
```

### Configuration

**Worker Settings** (Cloudflare Dashboard â†’ Workers â†’ Your Worker â†’ Settings):
- **CPU Limit**: 50ms (sufficient for most operations)
- **Memory**: 128MB
- **Cron Triggers**: Configured via `wrangler.toml` (`*/5 * * * *`)

**Pages Settings** (Cloudflare Dashboard â†’ Pages â†’ Your Project â†’ Settings):
- **Build command**: `cd packages/app && pnpm install && pnpm build`
- **Build output directory**: `packages/app/dist`
- **Environment variables**: `VITE_API_URL` (set to your Worker URL)

**Custom Domains:**

```bash
# Add custom domain to Worker
npx wrangler domains add api.example.com

# Update CORS_ORIGIN secret to include frontend domain
npx wrangler secret put CORS_ORIGIN
# Enter: https://feed.example.com
```

**Example:** `npx wrangler domains add api.tuvix.app`

### Monitoring & Troubleshooting

#### View Logs

```bash
# API (Workers) logs
npx wrangler tail

# Filter by status
npx wrangler tail --status error
npx wrangler tail --status ok

# Search logs
npx wrangler tail --search "RSS fetch"
npx wrangler tail --search "Cron triggered"

# Frontend (Pages) logs
# View in Cloudflare Dashboard â†’ Pages â†’ Your Project â†’ Deployments â†’ View logs
```

#### View Metrics

- **Workers**: Cloudflare Dashboard â†’ Workers â†’ Your Worker â†’ **Metrics**
- **Pages**: Cloudflare Dashboard â†’ Pages â†’ Your Project â†’ **Analytics**

#### Health Checks

```bash
# Check API health
curl https://api.example.com/health
# Or: curl https://your-worker.workers.dev/health
# Response: {"status":"ok","runtime":"cloudflare"}

# Check frontend
curl https://feed.example.com/health
# Or: curl https://your-pages-project.pages.dev/health
```

#### Common Issues

**CORS Errors:**
```bash
# Ensure CORS_ORIGIN includes your frontend URL
npx wrangler secret put CORS_ORIGIN
# Enter: https://feed.example.com
# Or if using Pages default: https://your-pages-project.pages.dev
```

**Authentication Cookies Not Working:**
- If frontend/API on different subdomains: Set `COOKIE_DOMAIN` secret
- Verify `CORS_ORIGIN` includes frontend URL
- Ensure frontend uses `credentials: "include"` in fetch requests

**Database Migration Failed:**
```bash
# Check migration status
npx wrangler d1 migrations list tuvix

# Re-run migrations
pnpm run db:migrate:d1

# Check D1 status
npx wrangler d1 execute tuvix --remote \
  --command "SELECT * FROM __drizzle_migrations;"
```

**Rate Limit Namespaces Not Found:**
- Verify `wrangler.toml` has correct format:
  - Uses `name` (not `binding`)
  - Uses `namespace_id` as a string integer (e.g., `"1001"`)
  - Uses `simple` object with `limit` and `period`
- Ensure `namespace_id` values are unique positive integers
- Check that bindings match the names used in code (`API_RATE_LIMIT`, `FEED_RATE_LIMIT`)

**Rate Limiting:**
- **API Rate Limiting**: Per-user, per-minute limits based on subscription plan
- **Public Feed Rate Limiting**: Per-feed owner, per-minute limits
- Monitor: `npx wrangler tail --search "Rate limit"`

---

## Shared Topics

### Environment Variables

#### Shared Variables (Both Deployments)

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `BETTER_AUTH_SECRET` | Yes | - | Secret for Better Auth session management (min 32 chars) |
| `CORS_ORIGIN` | Yes | - | Allowed CORS origins (comma-separated) |
| `NODE_ENV` | No | development | Environment mode |

#### Docker-Only Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATABASE_PATH` | No | ./data/tuvix.db | Path to SQLite database |
| `PORT` | No | 3001 | API server port |

#### Cloudflare-Only Variables

**Bindings** (configured in `wrangler.toml`):

| Binding | Type | Description |
|---------|------|-------------|
| `DB` | D1 | Database binding |
| `API_RATE_LIMIT` | RateLimit | API rate limiting binding |
| `FEED_RATE_LIMIT` | RateLimit | Public feed rate limiting binding |

**Secrets** (set via `wrangler secret put` - **not in wrangler.toml**):

| Secret | Required | Description |
|--------|----------|-------------|
| `BETTER_AUTH_SECRET` | Yes | Secret for Better Auth session management (min 32 chars) |
| `CORS_ORIGIN` | Yes | Allowed CORS origins (comma-separated) |
| `RESEND_API_KEY` | No | Resend API key for email service (see [Email System Guide](developer/email-system.md)) |
| `EMAIL_FROM` | No | Email sender address (must match verified domain in Resend) |
| `BASE_URL` | No | Base URL for generating email links |
| `COOKIE_DOMAIN` | No | Root domain for cross-subdomain cookies (e.g., "example.com") |
| `ADMIN_USERNAME` | No | Admin username for initialization |
| `ADMIN_PASSWORD` | No | Admin password for initialization |
| `ADMIN_EMAIL` | No | Admin email for initialization |

**âš ï¸ Security Note**: Never commit secrets to `wrangler.toml`. Use `wrangler secret put` for all sensitive values. The `wrangler.toml` file with empty IDs is safe to commit as a template.

### Database Migrations

#### Docker Compose Migrations

Migrations run automatically on container startup (packages/api/Dockerfile:45):

```bash
CMD ["sh", "-c", "node dist/db/migrate-local.js && node dist/adapters/express.js"]
```

**Manual migrations:**

```bash
# From host
pnpm run db:migrate

# From container
docker compose exec api node dist/db/migrate-local.js
```

#### Cloudflare D1 Migrations

Must be run manually before deployment:

```bash
cd packages/api

# Generate migration from schema changes
pnpm run db:generate

# Apply to local D1
pnpm run db:migrate:d1:local

# Apply to remote D1
pnpm run db:migrate:d1

# Verify migrations
npx wrangler d1 execute tuvix --remote \
  --command "SELECT name FROM sqlite_master WHERE type='table';"

# Check migration status
npx wrangler d1 migrations list tuvix
```

#### Migration Workflow

1. **Modify Schema** - Edit `packages/api/src/db/schema.ts`
2. **Generate Migration** - `pnpm run db:generate`
3. **Test Locally** - Run on local database
4. **Deploy**:
   - Docker: Restart containers (auto-migrates)
   - Workers: Run `pnpm run db:migrate:d1` then deploy

### Scheduled Tasks (Cron)

TuvixRSS runs two scheduled tasks:

1. **RSS Feed Fetching** - Fetches new articles from subscribed feeds
2. **Article Pruning** - Removes old articles based on retention policy

#### Docker Compose Cron

Uses `node-cron` (scheduler.ts:44):

```typescript
// RSS fetch - dynamic interval from global_settings
cron.schedule(fetchCronExpression, async () => {
  await handleRSSFetch(env);
});

// Article prune - daily at 2 AM
cron.schedule("0 2 * * *", async () => {
  await handleArticlePrune(env);
});
```

**Configuration:**
- Fetch interval: Configurable via `global_settings.fetchIntervalMinutes`
- Default: 60 minutes
- Minimum: 5 minutes

**Logs:**
```bash
docker compose logs -f api | grep "RSS fetch\|Prune"
```

#### Cloudflare Workers Cron

Uses Workers Scheduled Events (cloudflare.ts:284):

```toml
# wrangler.toml
[triggers]
crons = ["*/5 * * * *"]  # Every 5 minutes
```

**How it works:**
1. Cron triggers every 5 minutes
2. Checks `global_settings.lastRssFetchAt` and `fetchIntervalMinutes`
3. Runs RSS fetch if interval has elapsed
4. Checks `global_settings.lastPruneAt`
5. Runs prune if 24 hours have elapsed

**Configuration:**
```bash
# View cron triggers
npx wrangler deployments list

# Test cron locally
npx wrangler dev --test-scheduled

# Monitor cron execution
npx wrangler tail --search "Cron triggered"
```

**Cron Interval Limits:**
- Cloudflare: Minimum 1 minute intervals
- Recommended: 5-15 minutes (balance between freshness and costs)

#### Customizing Fetch Interval

Both deployments read from `global_settings` table:

```sql
-- Update via SQL
UPDATE global_settings SET fetchIntervalMinutes = 30 WHERE id = 1;
```

Or via admin UI:
1. Navigate to Settings
2. Update "Fetch Interval (minutes)"
3. Save

---

## CI/CD Integration

TuvixRSS uses GitHub Actions for automated CI/CD with a three-branch workflow: `feature` â†’ `development` â†’ `main`.

### Branch Flow

```
feature branch â†’ PR â†’ development â†’ PR â†’ main â†’ Release â†’ Deploy
```

### Workflows

#### 1. CI - Feature Branch (`ci-feature.yml`)
**Triggers:** Pull requests targeting `development`

**Validates:**
- Lint & format checks
- TypeScript type checking
- API and App tests (with coverage)
- Build verification

**Purpose:** Ensure code quality before merging to development branch.

#### 2. CI - Development Branch (`ci-development.yml`)
**Triggers:** Pull requests targeting `main`

**Validates:**
- All checks from feature branch workflow
- Coverage tracking and reporting
- Build artifact uploads

**Purpose:** Comprehensive validation before merging to main, including coverage tracking.

#### 3. Deploy to Cloudflare Workers (`deploy-cloudflare.yml`)
**Triggers:**
- Published GitHub releases (automatic)
- Manual workflow dispatch

**Process:**
1. Checks out release tag (from release or manual input)
2. Runs type checks and tests for API
3. Builds API
4. Substitutes `D1_DATABASE_ID` in `wrangler.toml` using `envsubst`
5. Deploys API to Cloudflare Workers
6. Runs database migrations (after successful API deployment)
7. Runs type checks and tests for App
8. Builds App (with `VITE_API_URL` from secrets)
9. Deploys App to Cloudflare Pages (after API deployment succeeds)
10. Outputs deployment summary with URLs

**Purpose:** Automated production deployment on releases.

### Required GitHub Secrets

Configure these in **Settings â†’ Secrets and variables â†’ Actions** â†’ **Secrets** tab:

| Secret | Required | Description |
|--------|----------|-------------|
| `CLOUDFLARE_API_TOKEN` | Yes | Cloudflare API token with Workers and Pages permissions |
| `CLOUDFLARE_ACCOUNT_ID` | Yes | Your Cloudflare account ID |
| `D1_DATABASE_ID` | Yes | Your D1 database ID (from `wrangler d1 create tuvix`) - used for envsubst substitution |
| `CLOUDFLARE_PAGES_PROJECT_NAME` | Yes | Cloudflare Pages project name |
| `VITE_API_URL` | Yes | API URL for frontend builds (e.g., `https://api.example.com/trpc` or `https://your-worker.workers.dev/trpc`) |

**Note:** The worker name is automatically read from `packages/api/wrangler.toml` â†’ `name` field. No secret needed.

**Getting Cloudflare Credentials:**

1. **API Token:** Cloudflare Dashboard â†’ My Profile â†’ API Tokens â†’ Create token with: Account.Cloudflare Workers:Edit, Account.Cloudflare Pages:Edit
2. **Account ID:** Found in Cloudflare Dashboard â†’ Right sidebar
3. **D1 Database ID:** Run `npx wrangler d1 create tuvix` locally, copy the `database_id` from output, add as `D1_DATABASE_ID` secret
4. **Pages Project:** Create via CLI (`npx wrangler pages project create tuvix-app`) or Dashboard, add project name as `CLOUDFLARE_PAGES_PROJECT_NAME` secret (must match exactly, case-sensitive)
5. **Worker Name:** Automatically read from `packages/api/wrangler.toml` â†’ `name` field (no secret needed)

### Deployment Process

#### Automatic Deployment (Recommended)

1. **Create a Release:**
   ```bash
   # Tag and push
   git tag v1.0.0
   git push origin v1.0.0
   
   # Or create release in GitHub UI
   # GitHub â†’ Releases â†’ Draft a new release
   ```

2. **Workflow Automatically:**
   - Checks out the release tag
   - Runs type checks and tests for API
   - Builds API
   - Substitutes `D1_DATABASE_ID` in `wrangler.toml` using `envsubst`
   - Deploys API to Cloudflare Workers
   - Runs database migrations (after API deployment succeeds)
   - Runs type checks and tests for App
   - Builds App with `VITE_API_URL` from secrets
   - Deploys App to Cloudflare Pages (only after API deployment succeeds)
   - Outputs deployment summary with URLs

#### Manual Deployment

1. Go to **Actions â†’ Deploy to Cloudflare Workers**
2. Click **"Run workflow"**
3. Select branch and enter version tag (e.g., `v1.0.0`)
4. Click **"Run workflow"**

### Workflow Features

- âœ… **Sequential Deployment:** API deploys first, then App (ensures API is ready)
- âœ… **Environment Variable Substitution:** `D1_DATABASE_ID` automatically substituted via `envsubst` before deployment
- âœ… **Validation:** Type checks and tests run before deployment
- âœ… **Database Migrations:** Automatically run after successful API deployment
- âœ… **Concurrency Control:** Prevents duplicate runs
- âœ… **Caching:** Optimized dependency caching
- âœ… **Environment Protection:** Production deployments use GitHub environments
- âœ… **Release Tag Checkout:** Ensures correct code version is deployed
- âœ… **Deployment URLs:** Displayed in workflow summary

### Troubleshooting CI/CD

**Workflow Fails:**
- Check Actions tab for specific error messages
- Verify all required secrets are configured (see Required GitHub Secrets above)
- Run checks locally: `pnpm run pre-check`
- Check that `D1_DATABASE_ID` substitution succeeded (look for "Successfully substituted" message)

**Deployment Fails:**
- Verify Cloudflare API token permissions (Workers:Edit, Pages:Edit)
- Check that Worker and Pages projects exist
- Verify `D1_DATABASE_ID` secret is set correctly (workflow will fail if missing)
- Review Cloudflare dashboard for errors
- Ensure database migrations completed successfully (runs after API deployment)
- Check that worker name in `wrangler.toml` matches your Cloudflare Worker

**Coverage Not Showing:**
- Coverage generates automatically during test runs
- Check that `coverage/lcov.info` files exist
- For private repos, may need `CODECOV_TOKEN` secret

See [`.github/workflows/README.md`](../.github/workflows/README.md) for detailed setup instructions.

---

## Security Checklist

### Both Deployments

- [ ] Generate strong BETTER_AUTH_SECRET (min 32 chars) for Better Auth session management
- [ ] Set restrictive CORS_ORIGIN
- [ ] Use HTTPS in production (required for secure cookies)
- [ ] Regular dependency updates (`pnpm update`)
- [ ] Monitor security advisories
- [ ] Regular database backups
- [ ] Verify Better Auth endpoints are accessible (`/api/auth/*`)

### Docker-Specific

- [ ] Don't commit `.env` to git
- [ ] Use Docker secrets for sensitive data
- [ ] Limit container resource usage
- [ ] Run containers as non-root user
- [ ] Regular security scans (`docker scan`)

### Workers-Specific

- [ ] Use Wrangler secrets (not vars) for sensitive data
- [ ] Enable Cloudflare security features (WAF, DDoS)
- [ ] Restrict Worker routes
- [ ] Monitor usage for cost control
- [ ] Review KV/D1 access logs

---

## Performance Optimization

### Docker

- Use multi-stage builds (already implemented)
- Mount volumes for data persistence
- Adjust health check intervals
- Use Docker build cache
- Consider using Alpine base images

### Workers

- Minimize Worker script size
- Use Smart Placement for reduced latency
- Configure appropriate CPU limits
- Use caching for static responses
- Monitor edge location performance

---

## Next Steps

After deployment:

1. Create admin user (see deployment sections above)
2. Configure global settings via admin UI
3. Set up monitoring and alerting
4. Configure backups
5. Test public RSS feeds
6. Add custom domain (optional)
7. Configure reverse proxy (Docker) or custom domain (Workers)

For more information, see:
- [README.md](../README.md) - Project overview
- [API README](../packages/api/README.md) - API documentation
- [App README](../packages/app/README.md) - Frontend documentation

---

**Last Updated:** 2025-01-15
